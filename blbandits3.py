import cvxpy as cvx
import numpy as np
from scipy.optimize import root, minimize
from numpy.linalg import norm, inv, slogdet
import scipy.linalg as sla
from numpy import exp
import scipy.linalg as sla
import numpy.random as ra
import numpy.linalg as la
import scipy.stats
import ipdb
from Functions.objective_functions import LinearModel
from scipy.special import logsumexp
from myutils3_v2 import *
import calcsubset
     
'''Place holder class that all GLM bandit algorithms must inherit from GLM '''

def gloc_solve_by_cvx(d,S,theta_prime,At):
    th = cvx.Variable(d)
    obj = cvx.Minimize(cvx.quad_form(th - theta_prime, At))
    cons = [cvx.norm(th) <= S]

    prob = cvx.Problem(obj, cons)
    prob.solve()

    return np.array(th.value).flatten()

def calc_sqrt_beta_det2(d,t,R,ridge,delta,S,logdetV):
    return R * np.sqrt( logdetV - d*log(ridge) + log (1/(delta**2)) ) + sqrt(ridge) * S

def calc_sqrt_beta_det3(t,R,ridge,delta,S,logdetV,logdetV0):
    """ to allow diagonal regularization
    """
    return R * np.sqrt( logdetV - logdetV0 + log (1/(delta**2)) ) + sqrt(ridge) * S

def calc_sqrt_beta_det4(t,R,ridge,delta,Sp,logdetV,logdetV0):
    """ to allow diagonal regularization, actually I can have a better one.
    `Sp = sqrt(ridge) * S` in the previous versions.
    """
    return R * np.sqrt( logdetV - logdetV0 + log (1/(delta**2)) ) + Sp


def calc_sqrt_beta_thompson(d,t,R,delta):
   return R * np.sqrt(9 * d * np.log(t/delta))

def mu_logistic(z):
    return 1.0/(1.0+np.exp(-z))

def mu_probit(z):
    return scipy.stats.norm.cdf(z)

def logistic_loss_pm1(w, x, y):
    y = float(y)
    assert (y in [+1.0, -1.0])
    z = y *np.dot(x,w)
    return np.log(1 + np.exp(-z))

def logistic_loss_01(w, x, y):
    assert (y in [0.0, 1.0])
    yy = y * 2 - 1
    return logistic_loss_pm1(w,x,yy)

def logistic_loss_pm1_grad(w, x, y):
    y = float(y)
    assert (y in [+1.0, -1.0])
    z = y * np.dot(x,w)
    return - 1.0 / (1 + np.exp(z)) * (y*x)

def logistic_loss_01_grad(w, x, y):
    assert (y in [0.0, 1.0])
    yy = y * 2 - 1
    return logistic_loss_pm1_grad(w,x,yy)

def mfunc_logistic(z):
    return np.log(1.0 + np.exp(z))

def mfunc_logistic_der(z):
    return 1.0/(1.0 + np.exp(-z))

from sklearn import metrics
def evalAuc(banditObj, dataObj, nTry=1):
    """
        evaluates deployment performance by AUC. 
        repeats are important when the data is synthetic and generated by p(y|x)
    """
    aucList = []
    trainAucList = []
    testAucList = []
    N = dataObj.N
    for iTry in range(nTry):
        #- 1. generate the labels
        y = np.array([dataObj.get_reward(i) for i in range(dataObj.N)])
        #- 2. let the bandit compute the scores (0.0-1.0)
        pred = banditObj.predict(dataObj.X)
        #- 3. normalize the pred to be in (0.0-1.0)
        mini = pred.min()
        maxi = pred.max()
        yhat = (pred - mini) / (maxi - mini)
        #- 4. measure auc
        auc = metrics.roc_auc_score(y, yhat)

        assert(np.all(dataObj.X == banditObj.X))
        do_not_ask = banditObj.getDoNotAsk()
        trainAuc = metrics.roc_auc_score(y[do_not_ask], yhat[do_not_ask])

        testIdx = np.setdiff1d(np.arange(N), do_not_ask)
        testAuc = metrics.roc_auc_score(y[testIdx], yhat[testIdx])

        aucList.append(auc)
        trainAucList.append(trainAuc)
        testAucList.append(testAuc)

    return np.mean(aucList), np.mean(trainAucList), np.mean(testAucList)

################################################################################
# bandit classes
################################################################################
class Bandit(object):
    def __init__(self, X, theta):
        raise NotImplementedError()

    def next_arm(self):
        raise NotImplementedError()

    def update(self):
        raise NotImplementedError()

    def get_debug_dict(self):
        raise NotImplementedError()

class Glm(object):
    def __init__(self):
        raise NotImplementedError()


class GlmLogistic(Glm):
    def __init__(self):
        raise NotImplementedError()

    @staticmethod
    def negloglik(z, y):
        assert (float(y) == 1.0 or float(y) == 0.0)
        return -z*y + GlmLogistic.m(z)

    @staticmethod
    def negloglik_derivative(z, y):
        assert (float(y) == 1.0 or float(y) == 0.0)
        return -y + GlmLogistic.mu(z)

    @staticmethod
    def mu(z):  # link function 
        return 1.0/(1.0+np.exp(-z))

    @staticmethod
    def m(z):   # integral of mu 
        return logsumexp([np.zeros(z.shape), z],axis=0) #np.log(1.0 + np.exp(z))
#        return np.log(1.0 + np.exp(z))

    @staticmethod
    def getKappa(S):
        return 1.0 / ((1 + exp(S)) * (1 + exp(-S)))

class GlmProbit(Glm):
    def __init__(self):
        raise NotImplementedError()

    @staticmethod
    def negloglik(z, y):
        assert (float(y) == 1.0 or float(y) == 0.0)
        return -z*y + GlmProbit.m(z)

    @staticmethod
    def negloglik_derivative(z, y):
        assert (float(y) == 1.0 or float(y) == 0.0)
        return -y + GlmProbit.mu(z)

    @staticmethod
    def mu(z):  # link function
        return scipy.stats.norm.cdf(z)

    @staticmethod
    def m(z):   # integral of mu
        return z*sstats.norm.cdf(z) + sstats.norm.pdf(z)

    @staticmethod
    def getKappa(S):
        return scipy.stats.norm.pdf(S)

    pass


class GlmGaussian(Glm):
    def __init__(self):
        raise NotImplementedError();

    @staticmethod
    def negloglik(z, y):
        return -z*y + GlmGaussian.m(z);

    @staticmethod
    def negloglik_derivative(z, y):
        return -y + GlmGaussian.mu(z);

    @staticmethod
    def mu(z):  # link function 
        return z

    @staticmethod
    def m(z):   # integral of mu 
        return .5*z**2

    @staticmethod
    def getKappa(S):
        return 1.0

########################################
class BilinearGlocNuclear(Bandit):
########################################
    """
    note that this is just for the squared loss (1/2)*(theta^T * x - y)^2
    R: subgaussian parameter.
    S: norm upper bound of theta_star
    r: range parameter for qgloc
    kappa: lower bound on the dervative of \mu, always 1 for squared loss.
    """    
    def __init__(self, X, Z, lam, R, S_star, glm=GlmGaussian, flags={},
                 multiplier=1.0, calc_radius_version=3, bArmRemoval=False):
        self.X = X
        self.Z = Z
        self.lam = lam
        self.R = R
        self.S_star = S_star
        self.glm = glm
        self.kappa = glm.getKappa(self.S_star)
        assert self.kappa == 1.0
        self.multiplier = multiplier
        self.bArmRemoval = bArmRemoval
        self.bNaive = flags.get('bNaive', False)

        #- more instance variables
        self.t = 1
        self.N1, self.d1 = self.X.shape
        self.N2, self.d2 = self.Z.shape

        # super arms
        self.N = self.N1 * self.N2
        self.d = self.d1 * self.d2
        self.W = np.zeros( (self.N, self.d) )
        for i in range(self.W.shape[0]):
            i1, i2 = np.unravel_index(i, (self.N1, self.N2))
            self.W[i,:] = np.outer(self.X[i1,:], self.Z[i2,:]).ravel()

        self.At = np.eye(self.d)*self.lam
        self.invAt = np.eye(self.d)/self.lam
        self.theta_s = np.zeros(self.d); 
        # print "Note that I am setting theta_s as zeros and this is warned to cause an error in cvx."
        self.theta_hat = self.theta_s.copy()
        #- theta_hat = np.zeros(self.d); # this causes an error in cvx
        #- alternative: I could randomize with just a bit of noise

        #-- TODO I could use the following to speed up
        # self.W_invVt_norm_sq = np.sum(self.W * self.W, axis=1) / self.lam

        self.WTq = np.zeros(self.d)
        self.sum_q_t_sq = 0
        self.radius_problem_constant = 0.

        self.cvx_th = cvx.Variable(self.d)
        #- WARNING: it is important to reshape by (d2,d1) not (d1,d2) 
        self.cvx_cons = [cvx.norm(cvx.reshape(self.cvx_th, (self.d2,self.d1)), 'nuc') <= self.S_star]

        #- do_not_ask_list
        assert bArmRemoval == False
        self.do_not_ask = []

        self.dbg_dict = {
            'multiplier':float(multiplier),
            'calc_radius_version': calc_radius_version,
        }

        self.time_obj = 0.0
        self.time_cvx = 0.0

        self.cvx_th_p = cvx.Parameter(shape=self.d)
        self.cvx_A = cvx.Parameter(shape=(self.d,self.d), PSD=True) 
        self.cvx_obj = cvx.Minimize(cvx.quad_form(self.cvx_th - self.cvx_th_p, self.cvx_A))
        self.cvx_prob = cvx.Problem(self.cvx_obj, self.cvx_cons)

    def _calc_radius_sq_v1(self): # the best at around 1.0
        if (self.t == 1):
            radius_sq = 0
        else:
            radius_sq = self.radius_problem_constant + (self.S_star**2) * self.lam
        return radius_sq

    def _calc_radius_sq_v2(self): # the best at around 1.0
        if (self.t == 1):
            radius_sq = 0
        else:
            radius_sq = self.radius_problem_constant
        return radius_sq

    def _calc_radius_sq_v3(self): # the best at around 0.01
        dt = 0.2; R = self.R
        if (self.t == 1):
            radius_sq = 0
        else:
            #- still, omitting some terms.
            B = (0.5/self.kappa)*self.radius_problem_constant + 2*self.kappa*(self.S_star**2) * self.lam
            inner = 1 + (2/self.kappa)*B + 4*R**4/(self.kappa**4 * dt**2)
            extra = (self.sum_q_t_sq - self.theta_hat.dot(self.WTq))
            assert (extra > -1e-8)
            alpha = 1 + (4.0/self.kappa)*B \
                      + (8*R**2/self.kappa**2)*np.log((2/dt)*np.sqrt( inner ))
            radius_sq =  alpha\
                         + self.lam*self.S_star**2 \
                         - extra
        return radius_sq

    def _calc_radius_sq(self): 
        v = self.dbg_dict['calc_radius_version']
        if (v == 1):
            return self._calc_radius_sq_v1()
        elif (v == 2):
            return self._calc_radius_sq_v2()
        elif (v == 3):
            return self._calc_radius_sq_v3()
        else:
            raise ValueError()

    def next_arm(self):
        valid_idx = setdiff1d(np.arange(self.N),self.do_not_ask)
        if (self.t == 1):
            return (ra.randint(self.N1), ra.randint(self.N2)), np.nan
        invAt = self.invAt
        radius_sq = self.multiplier * self._calc_radius_sq()

        tt = tic() # this is only 0.168 while time_cvx is 17.x seconds!!
        obj_func = np.dot(self.W, self.theta_hat)  \
                + np.sqrt(radius_sq) * np.sqrt(mahalanobis_norm_sq_batch(self.W, self.invAt))
        self.time_obj += toc(tt)

        arm_inner = np.argmax(obj_func[valid_idx])
        arm = valid_idx[arm_inner]

        chosenPair = np.unravel_index(arm, (self.N1,self.N2))
        return chosenPair, radius_sq

    def _solve_by_cvx(self, theta_prime, At):
        """
        if you want to improve the speed, might want to use quadprog. see
        https://scaron.info/blog/quadratic-programming-in-python.html
        """
        d = self.d
        th = self.cvx_th
        cons = self.cvx_cons

        #- new method
        obj = self.cvx_obj
        prob = self.cvx_prob
        self.cvx_th_p.value = theta_prime
        self.cvx_A.value = At

        if self.bNaive:
            return theta_prime
        else:
            #- previous method
            # obj = cvx.Minimize(cvx.quad_form(th - theta_prime, At))
            # prob = cvx.Problem(obj, cons)
            try: 
                tt = tic()
    #            prob.solve(warm_start=True, solver="MOSEK")
                #- eps=1e-5 is the default
                prob.solve(warm_start=True, solver="SCS", eps=1e-2)
                self.time_cvx += toc(tt)
            except Exception as inst:
                print('#'*40); print('# ' + str(inst)); print('#'*40)
                print('try again, with a different solver')
                try:
                    prob.solve(solver=cvx.SCS)
                except Exception as inst2:
                    ipdb.set_trace()
                    pass

            sol = np.array(th.value).flatten()
            assert sol[0] is not None
            return sol
    
    def update(self, pulled_idx_pair, y_t):
        pulled_idx = np.ravel_multi_index(pulled_idx_pair, (self.N1,self.N2))
        wt = self.W[pulled_idx, :]

        At = self.At
        invAt = self.invAt
        theta_s = self.theta_s
        kappa = self.kappa
        d = self.d

        At_new = At + np.outer(wt, wt)
        At_new = .5*(At_new + At_new.T)
        # eigvalsh = sla.eigvalsh(At_new)
        # assert min(eigvalsh) >= 0 and eigvalsh.dtype != complex
        invAt_new = inv(At_new)

        z = np.dot(theta_s, wt)
        grad = self.glm.negloglik_derivative(z, y_t); #  (1 / (1 + np.exp(-np.dot(theta_s, wt))) - y_t)
        theta_prime = theta_s - grad * np.dot(invAt_new, wt) / kappa

        try:    
            theta_s = self._solve_by_cvx(theta_prime, At_new)
        except Exception as inst:
            eps = 1e-7
            print('#'*40); print('# '+ str(inst)); print('#'*40)
            print('try again... by adding eps=%g'%eps)

            try:
                theta_s = self._solve_by_cvx(theta_prime, At_new + eps*np.eye(len(theta_prime)))
            except Exception as inst2:
                ipdb.set_trace()

        assert theta_s[0] is not None

        self.At = At_new
        self.invAt = invAt_new
        self.theta_s = theta_s

        #- extra
        qt = np.dot(theta_s, wt)
        self.WTq += qt * wt
        self.sum_q_t_sq += qt**2
        invAt = self.invAt
        self.radius_problem_constant += (grad ** 2) * np.dot(wt, np.dot(invAt, wt)); # no need for this, but let's keep it this way.
        self.theta_hat = np.dot(self.invAt, self.WTq)
        if (self.bArmRemoval):
            self.do_not_ask.append( pulled_idx_pair )

        self.t += 1
    
    def getDoNotAsk(self):
        return self.do_not_ask

    def predict(self, X=None):
        raise NotImplementedError()
        if X is None:
            X = self.X
        return X.dot(self.theta_hat)

    def get_debug_dict(self):
        return self.dbg_dict

    pass

########################################
class BilinearOful(Bandit):
########################################
    """ this is now heavily modified for graph bandits.. for the original, please use the one in 'expr-nips17-post'
    Sp: 'S prime', for oful, should be \sqrt(lam) * (2-norm bound on theta) 
        For spectral bandit, should be a bound on ||\\theta||_{V_0}
    """
    def __init__(self, X, Z, lam, R, Sp, D=None, flags={}, subsample_func=None, subsample_rate=1.0, multiplier=1.0, binaryRewards=False, bArmRemoval=False):
        """
        D allows diagonal regularization.
        Warning: Sp must be set to `sqrt(lam) * S` for OFUL.
        """
        self.X = X
        self.Z = Z
        self.R = R
        self.lam = lam
        self.delta = .2
# 		self.S_frobnorm = S_frobnorm
# 		self.Sp = np.sqrt(self.lam) * self.S_frobnorm
        self.Sp = Sp
        self.flags = flags
        self.multiplier = float(multiplier)
        self.binaryRewards = binaryRewards
        self.bArmRemoval = bArmRemoval

        # more instance variables
        self.t = 1
        self.N1, self.d1 = self.X.shape
        self.N2, self.d2 = self.Z.shape

        # super arms
        self.N = self.N1 * self.N2
        self.d = self.d1 * self.d2
        self.W = np.zeros( (self.N, self.d) )
        for i in range(self.W.shape[0]):
            i1, i2 = np.unravel_index(i, (self.N1, self.N2))
            self.W[i,:] = np.outer(self.X[i1,:], self.Z[i2,:]).ravel()

        #- subsampling aspect (disabled for now)
        assert subsample_func == None
        self.subsample_func = None

        self.WTy = np.zeros(self.d)

        self.D = D
        if (self.D is None):
            self.Vt = self.lam * np.eye(self.d)
            self.invVt = np.eye(self.d) / self.lam
            self.W_invVt_norm_sq = np.sum(self.W * self.W, axis=1) / self.lam
            self.logdetV = self.d*log(self.lam)
        else:
            assert self.lam is None
            self.Vt = D
            self.invVt = np.diag(1/np.diag(self.Vt))
            self.W_invVt_norm_sq = np.sum((self.W*np.diag(self.invVt)) * self.W, axis=1)
            self.logdetV = np.sum(np.log(np.diag(self.Vt)))
        self.logdetV0 = self.logdetV

        self.sqrt_beta = calc_sqrt_beta_det4(self.t,self.R,self.lam,self.delta,self.Sp,self.logdetV, self.logdetV0)
        self.theta_hat = np.zeros(self.d)

        assert bArmRemoval == False # let's implement this later on.
        self.do_not_ask = []
        self.dbg_dict = {'multiplier':float(multiplier)}
        self.cache_valid_idx = np.arange(self.N)

    #@profile
    def next_arm(self):
        if (len(self.do_not_ask) == 0):
            valid_idx = self.cache_valid_idx
        else:
            valid_idx = setdiff1d(np.arange(self.N),self.do_not_ask)
        if (self.t == 1):
            return (ra.randint(self.N1), ra.randint(self.N2)), np.nan
        radius_sq = self.multiplier * (self.sqrt_beta)**2
        if (self.subsample_func == None):
#            obj_func = np.dot(self.W, self.theta_hat) + np.sqrt(radius_sq) * np.sqrt(self.W_invVt_norm_sq)
#            A = np.dot(self.W, self.theta_hat)
            A = (self.X @ self.theta_hat.reshape(self.d1,self.d2) @ self.Z.T).ravel() 
            B = np.sqrt(radius_sq) * np.sqrt(self.W_invVt_norm_sq)
            obj_func = A + B
            if (len(self.do_not_ask) == 0):
                chosen = np.argmax(obj_func)
            else:
                chosen_inner = np.argmax(obj_func[valid_idx])
                chosen = valid_idx[chosen_inner]
        else:
            raise NotImplementedError("use valid_idx")

        chosenPair = np.unravel_index(chosen, (self.N1,self.N2))
        return chosenPair, radius_sq

    def calc_index(self):
        """ newly written for `SpectralUCB`
        """
        radius_sq = self.multiplier * (self.sqrt_beta)**2
        obj_func = np.dot(self.W, self.theta_hat) + np.sqrt(radius_sq) * np.sqrt(self.W_invVt_norm_sq)
        if (self.bArmRemoval):
            obj_func[self.do_not_ask] = -np.inf
        return obj_func

    def update(self, pulled_idx_pair, y_t):
        pulled_idx = np.ravel_multi_index(pulled_idx_pair, (self.N1,self.N2))
        wt = self.W[pulled_idx, :]

        if (self.binaryRewards):
            assert (y_t >= 0.0 and y_t <= 1.0)
            self.WTy += (2*y_t - 1) * wt
        else:
            self.WTy += y_t*wt
        self.Vt += np.outer(wt,wt)

        tempval1 = np.dot(self.invVt, wt)    # d by 1, O(d^2)
        tempval2 = np.dot(tempval1, wt)      # scalar, O(d)
        self.logdetV += log(1 + tempval2)

        if (self.t % 100 == 0):
            self.invVt = la.inv(self.Vt)
        else:
#            self.invVt -= np.outer(tempval1, tempval1) / (1 + tempval2)
            aVec = tempval1 / np.sqrt(1 + tempval2)
            self.invVt -= np.outer(aVec, aVec)

        if (self.subsample_func == None):
            # self.W_invVt_norm_sq = mahalanobis_norm_sq_batch(self.W, self.invVt)  # O(Nd^2) 
#            self.W_invVt_norm_sq -= (np.dot(self.W, tempval1) ** 2) / (1 + tempval2) # efficient update, O(Nd)
            v = (np.dot(self.X, tempval1.reshape(self.d1,self.d2)) @ self.Z.T).ravel()
            self.W_invVt_norm_sq -= (v ** 2) / (1 + tempval2) # efficient update, O(Nd)
            pass

        self.theta_hat = np.dot(self.invVt, self.WTy)
        if (self.bArmRemoval):
            self.do_not_ask.append( pulled_idx_pair )

        my_t = self.t + 1
        self.sqrt_beta = calc_sqrt_beta_det4(my_t,self.R,self.lam,self.delta,self.Sp,self.logdetV,self.logdetV0)

        self.t += 1

    def getDoNotAsk(self):
        return self.do_not_ask

    def predict(self, X=None):
        raise NotImplementedError()
        if X is None:
            X = self.X
        return X.dot(self.theta_hat)

    def get_debug_dict(self):
        return self.dbg_dict


#- some functions necessary for the next class
def averageMatrixEntries(armPairs, rewards):
    matDict = {}; cntDict = {}
    for i in range(armPairs.shape[0]):
        [r,c] = armPairs[i,:]
        matDict[(r,c)] = matDict.get((r,c), 0.0) + rewards[i]
        cntDict[(r,c)] = cntDict.get((r,c), 0) + 1
    for ((r,c),v) in matDict.items():
        matDict[(r,c)] = v / cntDict[(r,c)]
    return matDict

########################################
class BilinearTwoStage(Bandit):
########################################
    """ this is now heavily modified for graph bandits.. for the original, please use the one in 'expr-nips17-post'
    """
    def __init__(self, X, Z, lam, R, S_F, sval_max, sval_min, r, C_T1, T, flags={}, subsample_func=None, subsample_rate=1.0, multiplier=1.0, binaryRewards=False, bArmRemoval=False, SpType=None, algoMatrixCompletion='optspace'):
        """
        Two stage: internally uses BilinearOful object.
        """
        self.X = X
        self.Z = Z
        self.R = R
        self.lam = lam
        self.delta = .2
        self.S_F = S_F
        self.sval_max = sval_max
        self.sval_min = sval_min
        self.r = r
        self.C_T1 = C_T1
        self.T = T
        self.flags = flags
        self.multiplier = float(multiplier)
        self.binaryRewards = binaryRewards
        self.bArmRemoval = bArmRemoval
        self.SpType = SpType
        self.algoMatrixCompletion = algoMatrixCompletion

        #- to be set in the first stage
        self.stage1arms = None
        self.stage1rewards = []
        self.hatUFull = None; self.hatVFull = None
        self.lamp = None
        self.Sp = None
        self.oful = None
        self.subsetX = None
        self.subsetZ = None

        # more instance variables
        self.t = 1
        self.N1, self.d1 = self.X.shape
        self.N2, self.d2 = self.Z.shape
        assert (self.d1 == self.d2)
        d = self.d1     # FIXME I should change this

        #- old scheme; saved for reference..
        # myT1 = int(np.ceil(C_T1 * self.R * d**(3/2) * r**(1/2) * np.sqrt(self.T)))
        # self.T1 = np.maximum( myT1, self.r*(self.d1 + self.d2) - self.r**2 )

        #- new scheme
        minT1 = self.r*(self.d1 + self.d2) - self.r**2 
        self.T1 = int(np.ceil(C_T1 * minT1))

        self.dbg_dict = {}

    def next_arm(self):
        if (self.t == 1):
            # prepare representative arms.
            self.subsetX, self.invX_norm = calcsubset.hybrid(self.X, 20)
            self.subsetZ, self.invZ_norm = calcsubset.hybrid(self.Z, 20)
            self.subsetXInv = dict(zip(self.subsetX, range(len(self.subsetX))))
            self.subsetZInv = dict(zip(self.subsetZ, range(len(self.subsetZ))))

            mat = dstack_product(self.subsetX, self.subsetZ)   # num of super arms (d^2) by 2
            idxAry = ra.permutation(mat.shape[0])
            nRepeat = int((self.T1 - 1) // len(idxAry) + 1)
            idxAry = np.squeeze(np.tile(idxAry, (1,nRepeat)))
            self.stage1arms = mat[idxAry[:self.T1],:]
            self.stage1rewards = nans(self.T1)

            #- if there is an empty row / column, then ensure that there is no empty row/column.
            sa = self.stage1arms
            if len(np.unique(sa[:,0])) != self.d1 or len(np.unique(sa[:,1])) != self.d2:
                idxDiagonals = np.arange(0,len(mat),self.d2+1)
                idxRemainders = np.setdiff1d(range(len(mat)), idxDiagonals)
                idxRemainders = ra.permutation(idxRemainders)
                oneTile = np.concatenate( (idxDiagonals,idxRemainders) )

                nRepeat = int((self.T1 - 1) // len(idxAry) + 1)
                idxAry = np.squeeze(np.tile(oneTile, (1,nRepeat)))
                self.stage1arms = mat[idxAry[:self.T1],:]
            
            #- save necessary stats
            self.dbg_dict['T1'] = self.T1
            printExpr("self.T1")
            pass

        if (self.t <= self.T1):
            armPairToPull = tuple(self.stage1arms[self.t-1,:])
            radius_sq = np.nan
        else:
            # at the beginning of the second stage
            if (self.t == self.T1+1):
                #----- invoke matrix completion
                    #- average out entries observed more than once.; stage1arms: list of (lArmIdx,rArmIdx).
                matDict = averageMatrixEntries(self.stage1arms, self.stage1rewards)
                    #- translate index
                smat = [(self.subsetXInv[k1],self.subsetZInv[k2],v) for ((k1,k2),v) in matDict.items()]
                    #- run optspace and but catch the stdout
                if self.algoMatrixCompletion == 'optspace':
                    import optspace
                    [U,S,V,out_niter] = optspace.optspace(smat, rank_n=self.r, 
                                    num_iter=1000, 
                                    tol=1e-4, 
                                    verbosity=0, 
                                    outfile="")
                    printExpr('out_niter')
                    hatK = (U @ S @ V.T)
                    self.dbg_dict['out_niter'] = out_niter
                    assert np.all(np.logical_and(~np.isnan(hatK),~np.isinf(hatK)))
                elif self.algoMatrixCompletion == 'bm':
                    myX = []; myZ = []; myRewards = []
                    for ((k1,k2),v) in matDict.items():
                        myX.append( indicator(self.subsetXInv[k1],self.d1) )
                        myZ.append( indicator(self.subsetZInv[k2],self.d2) )
                        myRewards.append( v )
                    myX = np.array(myX) 
                    myZ = np.array(myZ)
                    myRewards = np.array(myRewards)

                    from matrixrecovery import rankone
                    U,V,out_nIter,stat = rankone(myX,myZ,myRewards,self.r,self.R)
                    printExpr('out_nIter')
                    hatK = U@V.T
                    self.dbg_dict['out_nIter'] = out_nIter
                    assert np.all(np.logical_and(~np.isnan(hatK),~np.isinf(hatK)))
                else:
                    raise ValueError()

                #- Instead of the following, we do a robust version of the same operation
                #- hatTh = la.inv(self.X[self.subsetX,:]) @ hatK @ la.inv(self.Z[self.subsetZ,:].T) 
                #- note lstsq is like solve(), but solves approximately when ill-conditioned
# 				tmp = la.solve(self.X[self.subsetX,:], hatK)
# 				self.hatThStage1 = la.solve(self.Z[self.subsetZ,:], tmp.T).T
                tmp, _,_,_ = la.lstsq(self.X[self.subsetX,:], hatK, rcond=None)
                tmp2, _,_,__ = la.lstsq(self.Z[self.subsetZ,:], tmp.T, rcond=None)
                self.hatThStage1 = tmp2.T

                #- get the subspaces
                [self.hatUFull, self.hatSFull, VT] = la.svd(self.hatThStage1)
                self.hatVFull = VT.T

                #- rotate the arms
                self.newX = self.X @ self.hatUFull
                self.newZ = self.Z @ self.hatVFull

                #- prepare oful
                d = self.d1 # FIXME just an impromptu
                T2 = self.T - self.T1
                self.lamp = T2/d/self.r/np.log(1+T2/self.lam) # FIXME I think I should use T rather than T2...
                term1 = np.sqrt(self.lam) * self.S_F #2 * np.sqrt(d*self.r)

                kappa = self.sval_max / self.sval_min
                Cp_Cpp = 1
                term2 = np.sqrt(self.lamp)  \
                      * Cp_Cpp**2 * kappa**4 * self.R**2 * d**3 * self.r / self.sval_min**2 / self.T1 \
                      * self.invX_norm**2 * self.invZ_norm**2

                if   self.SpType == None:
                    self.Sp = term1 + self.sval_max * term2
                elif self.SpType == 'simple':
                    term2p = np.sqrt(self.lamp)  \
                      * self.R**2 * d**3 * self.r / self.T1 \
                      * self.invX_norm**2 * self.invZ_norm**2 
                    self.Sp = term1 + self.sval_max * term2p
                elif self.SpType == 'simple2':
                    term2pp = np.sqrt(self.lamp)  \
                      * self.R**2 * d**3 * self.r / self.T1
                    self.Sp = term1 + self.sval_max * term2pp
                elif self.SpType == 'simple3':
                    self.Sp = term1 
                else:
                    raise ValueError()

                k = self.r*(self.d1 + self.d2) - self.r**2
                p = self.d1*self.d2
                diagvec = [self.lam]*(self.r * self.d2)
                row = [self.lam]*(self.r) + [self.lamp]*(self.d2 - self.r)
                diagvec += row*(self.d1 - self.r)
                self.D = np.diag(diagvec)                
#                self.D = np.diag([self.lam]*k + [self.lamp]*(p-k))

                #- initialize oful
                self.oful = BilinearOful(X=self.newX, Z=self.newZ, 
                                         lam=None, R=self.R, Sp=self.Sp, D=self.D,
                                         flags={}, multiplier=self.multiplier)

                #- pseudo play oful, so it is up to date!
                for myt in range(self.T1):
                    self.oful.update( tuple(self.stage1arms[myt,:]), self.stage1rewards[myt] )

            #- get the next arm from oful 
            armPairToPull, radius_sq = self.oful.next_arm()

        return armPairToPull, radius_sq

    def update(self, pulled_arm_pair, y_t):
        if (self.t <= self.T1):
            assert(pulled_arm_pair == tuple(self.stage1arms[self.t-1,:]))
            self.stage1rewards[self.t-1] = y_t
        else:
            self.oful.update(pulled_arm_pair, y_t)

        self.t += 1

    def getDoNotAsk(self):
        return self.do_not_ask

    def predict(self, X=None):
        raise NotImplementedError()
        if X is None:
            X = self.X
        return X.dot(self.theta_hat)

    def get_debug_dict(self):
        return self.dbg_dict

####################
class BilinearOneStage(Bandit):
########################################
    """ a heuristic method that keeps updating the subspace in every exponentially-space time steps
    SpType: 'simple2' or 'simple3'
    """
    def __init__(self, X, Z, lam, R, S_F, sval_max, sval_min, r, T, flags={}, subsample_func=None, subsample_rate=1.0, multiplier=1.0, binaryRewards=False, bArmRemoval=False, SpType='simple2'):
        self.X = X.astype(float)
        self.Z = Z.astype(float)
        self.R = R
        self.lam = lam
        self.delta = .2
        self.S_F = S_F
        self.sval_max = sval_max
        self.sval_min = sval_min
        self.r = r
        self.T = T
        self.flags = flags
        self.multiplier = float(multiplier)
        self.binaryRewards = binaryRewards # perhaps not being used
        self.bArmRemoval = bArmRemoval
        self.SpType = SpType  # how to form Sp..?
        self.subspaceUpdateBase = np.sqrt(2)

        #- to be set in the first stage
        self.arms = []
        self.rewards = []
        self.hatUFull = None; self.hatVFull = None
        self.lamp = None
        self.Sp = None
        self.oful = None

        # more instance variables
        self.t = 1
        self.N1, self.d1 = self.X.shape
        self.N2, self.d2 = self.Z.shape
        assert (self.d1 == self.d2)
        d = self.d1     # FIXME I should change this

        #- we update subspace after every t=knotList[i]
        T1 = self.r*(self.d1 + self.d2) - self.r**2 
        base = self.subspaceUpdateBase
        L = np.ceil(np.log(T/T1) / np.log(base))
        self.knotList = np.ceil(L*base ** np.arange(0, L)).astype(int)

        #- initialize oful
        self.oful = BilinearOful(X=self.X, Z=self.Z, 
                                lam=self.lam, R=self.R, Sp=np.sqrt(self.lam) * self.S_F,
                                flags={}, multiplier=self.multiplier)

        self.dbg_dict = {'knotList': self.knotList,
                         'out_nIter_list': [] }

        
    def next_arm(self):
        return self.oful.next_arm()

    def update(self, pulled_arm_pair, y_t):
        self.arms.append(pulled_arm_pair)
        self.rewards.append(y_t)
        self.oful.update(pulled_arm_pair, y_t)

        if (self.t in self.knotList):
            #- estimate subspace
            myX = self.X[[i[0] for i in self.arms],:]
            myZ = self.Z[[i[1] for i in self.arms],:]
            from matrixrecovery import rankone
            U,V,out_nIter,stat = rankone(myX,myZ,np.array(self.rewards),self.r,self.R)
            Th = U@V.T
            U,S,VT = la.svd(Th)
            V = VT.T
            self.dbg_dict['out_nIter_list'].append( out_nIter )

            #- rotate the arms
            newX = self.X @ U
            newZ = self.Z @ V

            #- restart oful
            d = self.d1 # FIXME just an impromptu
            # T2 = self.T - self.t
            self.lamp = self.T/d/self.r/np.log(1+self.T/self.lam) # different from TwoStage; I am using T instead of T2
            term1 = np.sqrt(self.lam) * self.S_F #2 * np.sqrt(d*self.r)

            if self.SpType == 'simple2':
                term2pp = np.sqrt(self.lamp)  \
                * self.R**2 * d**3 * self.r / self.t
                self.Sp = term1 + self.sval_max * term2pp
            elif self.SpType == 'simple3':
                self.Sp = term1 
            else:
                raise ValueError()

            k = self.r*(self.d1 + self.d2) - self.r**2
            p = self.d1*self.d2
            diagvec = [self.lam]*(self.r * self.d2)
            row = [self.lam]*(self.r) + [self.lamp]*(self.d2 - self.r)
            diagvec += row*(self.d1 - self.r)
            self.D = np.diag(diagvec)                

            #- initialize oful
            self.oful = BilinearOful(X=newX, Z=newZ, 
                                    lam=None, R=self.R, Sp=self.Sp, D=self.D,
                                    flags={}, multiplier=self.multiplier)

            #- pseudo play oful, so it is up to date!
            for myt in range(self.t):
                self.oful.update(self.arms[myt], self.rewards[myt])

            pass

        self.t += 1

    def getDoNotAsk(self):
        return self.do_not_ask

    def predict(self, X=None):
        raise NotImplementedError()
        if X is None:
            X = self.X
        return X.dot(self.theta_hat)

    def get_debug_dict(self):
        return self.dbg_dict

################################################################################
# for experiments
################################################################################
class DataForBilinearBandit(object):
    def __init__(self):
        raise NotImplementedError()

    def gen_data(self):
        raise NotImplementedError()

    def get_reward(self, idx_pair):
        raise NotImplementedError()

def genRandomFeatures(A, r, d):
    """
    A: N × N matrix. the rank is r.
    extract features of rows/cols of A so that
    A = F @ Th @ G.T, where F and G are N × d, and Th is d × d (and rank r)
    """
    U,S,VH = la.svd(A)
    S = S[:r]
    r = len(S)
    U = U[:,:r] * np.sqrt(S)
    V = VH.T
    V = V[:,:r] * np.sqrt(S)

    B = ra.randn(d,r)
    F = U @ la.pinv(B)
    D = ra.randn(d,r)
    G = V @ la.pinv(D)

    Th = B@D.T

    return F, Th, G

class MovieLense(DataForBilinearBandit):
    def __init__(self, filename, R): #, d=16, r=5):
        self.R = R
        self.filename = filename
        self.rawdata = LoadPickle(self.filename)
        self.M = self.rawdata['M']
        self.N1, self.N2 = self.M.shape
    
    def gen_features(self, d=16, r=5):
        self.d = d
        self.r = r
        self.X, self.Th, self.Z = genRandomFeatures(self.M, r,d)

        self.S_F = la.norm(self.Th, 'fro')
        self._save_expected_rewards()

    def _save_expected_rewards(self):
        self.expt_reward = (self.X @ self.Th) @ self.Z.T
        self.best_arm_pair = tuple(np.unravel_index(np.argmax(self.expt_reward),
                                              self.expt_reward.shape))

    def get_reward(self, idx_pair):
        x = self.X[idx_pair[0],:]
        z = self.Z[idx_pair[1],:]
        return x @ self.Th @ z + self.R * ra.normal(0,1)

    def get_best_reward(self):
        return self.expt_reward[self.best_arm_pair]

    def get_expected_reward(self, idx_pair):
        """ can also take idx_pair as a list of index pairs (list of tuples)
        """
        return [data.expt_reward[row[0],row[1]] for row in idx_pair]

    def get_expected_regret(self, idx_pair):
        """ can also take idx_pair as a list of index pairs (list of tuples)
        """
        x = self.best_arm_pair[0]
        z = self.best_arm_pair[1]
        return self.expt_reward[x,z] - self.expt_reward[idx_pair[0], idx_pair[1]]
        if type(idx_pair) is list:
            return self.expt_reward[x,z] - self.get_expected_reward(self, idx_pair)

    def __str__(self):
        return str(self.__dict__)
    pass


class SphericalGaussian(DataForBilinearBandit):
    def __init__(self, R, r):
        self.R = R
        self.r = r

    def set_X_Z(self, X, Z):
        self.X = X
        self.Z = Z
        [self.N1, self.d1] = X.shape
        [self.N2, self.d2] = Z.shape
        self.N = N1*N2
        self.d = d1*d2


    def gen_theta_star(self, S_2norm=1.0):
        self._gen_theta_star(S_2norm)
        self._save_expected_rewards_()

    def _gen_theta_star(self, S_2norm=1.0):
        self.S_2norm = S_2norm

        #- generate Th
        v = ra.normal(0,1,self.d1*self.d2)
        Th0 = np.reshape(v, (self.d1, self.d2));
        if (self.r != np.min([self.d1, self.d2])):
            #- FIXME this part is buggy; use V there is actually V.T... 
            #-       but I will keep this for reproducibility
            U,s,V = la.svd(Th0)
            Th0 = (U[:,:self.r] * s[:self.r]) @ V[:,:self.r].T
        Th0 = Th0 / la.norm(Th0,2) # normalize by its two norm
        self.Th = Th0 * self.S_2norm
        self.S_F = la.norm(self.Th, 'fro')

    def _save_expected_rewards(self):
        self.expt_reward = (self.X @ self.Th) @ self.Z.T
        self.best_arm_pair = tuple(np.unravel_index(np.argmax(self.expt_reward),
                                              self.expt_reward.shape))

    @staticmethod
    def _genRademacher(N,d):
        return 2 * ra.randint(2,size=(N,d)) - 1

    def gen_data(self, d1, d2, N1, N2, S=1.0, armtype="gaussian"):
        """ type could be 'gaussian' or 'rademacher'
        """
        [self.d1, self.d2] = [d1, d2]
        [self.N1, self.N2] = [N1, N2]
        self.S = S

        if (armtype == "gaussian"):
            #- generate X
            X = ra.normal(0,1,(self.N1, self.d1))
            norms = la.norm(X, axis=1)
            X /= norms.reshape(-1,1)

            #- generate X
            Z = ra.normal(0,1,(self.N2, self.d2))
            norms = la.norm(Z, axis=1)
            Z /= norms.reshape(-1,1)

            #- save expected rewards
            self._gen_theta_star(self.S) 
        elif armtype == "rademacher":
            X = self.__class__._genRademacher(self.N1, self.d1)
            Z = self.__class__._genRademacher(self.N2, self.d2)
            #- save expected rewards
            self._gen_theta_star(self.S) # this is being repeated, but I keep this for replicability
        elif armtype == "rademacher2":
            #- ensure that there exists the arm with the largest reward!
            self._gen_theta_star(self.S)
            X = self.__class__._genRademacher(self.N1, self.d1)
            Z = self.__class__._genRademacher(self.N2, self.d2)
            U,S,VT = la.svd(self.Th)
            V = VT.T
            #- implant (nearly) best arm
            i1 = ra.randint(self.N1)
            X[i1,:] = np.sign(U[:,0])
            i2 = ra.randint(self.N2)
            Z[i2,:] = np.sign(V[:,0])
        else:
            raise ValueError()
        self.X = X; self.Z = Z
        self._save_expected_rewards()

    def get_reward(self, idx_pair):
        x = self.X[idx_pair[0],:]
        z = self.Z[idx_pair[1],:]
        return x @ self.Th @ z + self.R * ra.normal(0,1)

    def get_best_reward(self):
        return self.expt_reward[self.best_arm_pair]

    def get_expected_reward(self, idx_pair):
        """ can also take idx_pair as a list of index pairs (list of tuples)
        """
        return [data.expt_reward[row[0],row[1]] for row in idx_pair]

    def get_expected_regret(self, idx_pair):
        """ can also take idx_pair as a list of index pairs (list of tuples)
        """
        x = self.best_arm_pair[0]
        z = self.best_arm_pair[1]
        return self.expt_reward[x,z] - self.expt_reward[idx_pair[0], idx_pair[1]]
        if type(idx_pair) is list:
            return self.expt_reward[x,z] - self.get_expected_reward(self, idx_pair)

    def __str__(self):
        return str(self.__dict__)


#@profile
def run_bilinear_bandit(learner, data_obj, T, initIdx=-1,timeList=[]):
    reward_ary = np.zeros(T)
    arm_pair_ary = np.zeros((T,2), dtype=int16)
    inst_regret = np.zeros(T)
    cum_regret = np.zeros(T)

    #- initial point, if given
    if initIdx != -1:
        learner.update(initIdx, 1)

    my_tt = tic()
    for t in range(1,T+1):
        #- choose the next arm
        next_arm_pair, radius_sq = learner.next_arm()

        #- get reward and update the model
        reward = data_obj.get_reward(next_arm_pair)
        learner.update(next_arm_pair, reward)

        #- save stats
        reward_ary[t-1] = reward
        arm_pair_ary[t-1,:] = next_arm_pair
        inst_regret[t-1] = data_obj.get_expected_regret(next_arm_pair)
        if (t == 1):
            cum_regret[t-1] = inst_regret[t-1]
        else:
            cum_regret[t-1] = cum_regret[t-2] + inst_regret[t-1]

        # if (t % 300 == 0):
        #     print('%.4g' % toc(my_tt))
        #     print('%.4g' % learner.time_cvx)
        #     ipdb.set_trace()
        #     pass

        #- print out stats
        if (t % 1000 == 0):
            timeSoFar = toc(my_tt)
            print(('t=%d, time=%.1f, radius_sq= %.4f, inst_reg=%.4f, cum_reg=%.4f' % \
                    (t, timeSoFar, radius_sq, inst_regret[t-1], cum_regret[t-1])))
            timeList.append( [t,timeSoFar] )
            sys.stdout.flush()

    return reward_ary, arm_pair_ary, learner.get_debug_dict()

def run_bilinear_bandit_time(bandit,data_obj,T,initIdx=-1):
    timeList = []
    reward_ary, arm_pair_ary, dbg_dict = run_bilinear_bandit(bandit,data_obj,T,initIdx, timeList=timeList)
    return reward_ary, arm_pair_ary, dbg_dict, timeList

